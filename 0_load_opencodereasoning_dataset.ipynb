{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4b62a-759c-4869-8dcf-a3854abdead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c13577-9260-44a2-b896-f98f39fb1e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset : nvidia/OpenCodeReasoning\n",
      "Configs : ['split_0', 'split_1']\n",
      "Columns : ['id', 'input', 'source', 'dataset', 'license', 'split', 'difficulty', 'solution']\n",
      "Output  : opencodereasoning_filtered.parquet\n"
     ]
    }
   ],
   "source": [
    "# # Download OpenCodeReasoning Dataset\n",
    "# This script downloads the OpenCodeReasoning dataset from Hugging Face\n",
    "# using STREAMING MODE to avoid caching the full dataset to disk.\n",
    "\n",
    "# ## Cell 1: Install dependencies\n",
    "# !pip install datasets huggingface_hub pandas pyarrow\n",
    "\n",
    "# ## Cell 2: Imports\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ## Cell 3: Configuration\n",
    "DATASET_NAME    = \"nvidia/OpenCodeReasoning\"\n",
    "CONFIGS         = [\"split_0\", \"split_1\"]\n",
    "COLUMNS_TO_KEEP = [\"id\", \"input\", \"source\", \"dataset\", \"license\", \"split\", \"difficulty\", \"solution\"]\n",
    "OUTPUT_PARQUET  = \"opencodereasoning_filtered.parquet\"\n",
    "BATCH_SIZE      = 5_000   # rows buffered in memory before flushing to parquet\n",
    "\n",
    "print(f\"Dataset : {DATASET_NAME}\")\n",
    "print(f\"Configs : {CONFIGS}\")\n",
    "print(f\"Columns : {COLUMNS_TO_KEEP}\")\n",
    "print(f\"Output  : {OUTPUT_PARQUET}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62840fd5-e1ed-42b6-9d36-6d436b1e0bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Streaming config: split_0 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d023361d3b47edb2569d77e7341b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c6e687e8c0451c84e3e5b00200310b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rows written so far: 565,000\n",
      "  Finished split_0. Total rows written: 567,850\n",
      "\n",
      "--- Streaming config: split_1 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d8d84600cf4684b0788b2b91f41eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rows written so far: 732,850\n",
      "  Finished split_1. Total rows written: 735,255\n"
     ]
    }
   ],
   "source": [
    "# ## Cell 4: Stream both configs and write directly to Parquet\n",
    "# streaming=True means rows are fetched on-the-fly — no full dataset cache written to disk.\n",
    "\n",
    "parquet_writer = None\n",
    "total_rows     = 0\n",
    "\n",
    "for config in CONFIGS:\n",
    "    print(f\"\\n--- Streaming config: {config} ---\")\n",
    "    ds_stream = load_dataset(DATASET_NAME, config, split=config, streaming=True)\n",
    "\n",
    "    batch = []\n",
    "    for row in ds_stream:\n",
    "        filtered_row = {col: row.get(col) for col in COLUMNS_TO_KEEP if col in row}\n",
    "        batch.append(filtered_row)\n",
    "\n",
    "        if len(batch) >= BATCH_SIZE:\n",
    "            table = pa.Table.from_pandas(pd.DataFrame(batch), preserve_index=False)\n",
    "            if parquet_writer is None:\n",
    "                parquet_writer = pq.ParquetWriter(OUTPUT_PARQUET, table.schema)\n",
    "            parquet_writer.write_table(table)\n",
    "            total_rows += len(batch)\n",
    "            print(f\"  Rows written so far: {total_rows:,}\", end=\"\\r\")\n",
    "            batch = []\n",
    "\n",
    "    # Flush any remaining rows for this config\n",
    "    if batch:\n",
    "        table = pa.Table.from_pandas(pd.DataFrame(batch), preserve_index=False)\n",
    "        if parquet_writer is None:\n",
    "            parquet_writer = pq.ParquetWriter(OUTPUT_PARQUET, table.schema)\n",
    "        parquet_writer.write_table(table)\n",
    "        total_rows += len(batch)\n",
    "\n",
    "    print(f\"\\n  Finished {config}. Total rows written: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09335950-ce1e-488d-9a8b-5deb80c9b8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Streaming complete!\n",
      "Total rows : 735,255\n",
      "Saved to   : opencodereasoning_filtered.parquet\n",
      "\n",
      "--- Reloading from Parquet for inspection ---\n",
      "Shape   : (735255, 8)\n",
      "Columns : ['id', 'input', 'source', 'dataset', 'license', 'split', 'difficulty', 'solution']\n",
      "\n",
      "First 5 rows:\n",
      "                              id                                                                                                    input   source       dataset   license split         difficulty                                                                                                     solution\n",
      "c0d4209f929db2b5bf3526a47a2520b0 Problem description.\\nVipul is a hardworking super-hero who maintains the bracket ratio of all the st... codechef code_contests cc-by-4.0 train UNKNOWN_DIFFICULTY T = int(input())\\nfor _ in range(T):\\n    s = input().strip()\\n    stack = []\\n    valid = True\\n    for ...\n",
      "5378dbbc2f9928bfe9b3a196e3a45a0b  The Chef likes to stay in touch with his staff. So, the Chef, the head server, and the sous-chef all... codechef code_contests cc-by-4.0 train                  1 import sys\\n\\ndef main():\\n    T = int(sys.stdin.readline())\\n    for _ in range(T):\\n        R = int(sys...\n",
      "5295c0a98f0025a377be5340304aaae3 Frank explained its friend Felman the algorithm of Euclides to calculate the GCD \\nof two numbers. Th... codechef code_contests cc-by-4.0 train             MEDIUM import math\\n\\nn = int(input())\\nfor _ in range(n):\\n    a_str, b_str = input().split()\\n    a = int(a_st...\n",
      "e2e75d9d7d47d6f22c7eb408f8911af8  A Little Elephant from the Zoo of Lviv likes lucky strings, i.e., the strings that consist only of t... codechef code_contests cc-by-4.0 train          interview import bisect\\n\\ndef count_balanced_substrings(s):\\n    n = len(s)\\n    count_4 = [0] * (n + 1)\\n    for ...\n",
      "06aaff0ac61dca6506122cd03fa73ab0  Given a string s. Can you make it a palindrome by deleting exactly one character? Note that size of ... codechef code_contests cc-by-4.0 train                  2 def is_palindrome(s):\\n    return s == s[::-1]\\n\\nT = int(input())\\nfor _ in range(T):\\n    s = input().s...\n",
      "\n",
      "--- Dataset Statistics ---\n",
      "\n",
      "Value counts — source:\n",
      "source\n",
      "codeforces       386948\n",
      "codechef          72925\n",
      "aizu              62476\n",
      "hackerearth       59181\n",
      "atcoder           47222\n",
      "geeksforgeeks     37602\n",
      "codewars          34326\n",
      "kattis            13095\n",
      "hackerrank        10955\n",
      "leetcode          10525\n",
      "\n",
      "Value counts — difficulty:\n",
      "difficulty\n",
      "UNKNOWN_DIFFICULTY    114214\n",
      "VERY_HARD             100807\n",
      "interview              97152\n",
      "EASY                   85175\n",
      "HARD                   68739\n",
      "introductory           50378\n",
      "MEDIUM                 36807\n",
      "MEDIUM_HARD            35024\n",
      "competition            23916\n",
      "0                      21707\n",
      "7                      19029\n",
      "8                      18259\n",
      "9                      13486\n",
      "11                     12944\n",
      "10                     12844\n",
      "12                      8323\n",
      "2                       5357\n",
      "3                       4708\n",
      "13                      2321\n",
      "1                       1900\n",
      "6                        905\n",
      "14                       362\n",
      "20                       260\n",
      "15                       172\n",
      "17                       127\n",
      "16                       126\n",
      "19                        69\n",
      "21                        54\n",
      "22                        40\n",
      "24                        38\n",
      "23                        12\n",
      "\n",
      "Value counts — dataset:\n",
      "dataset\n",
      "code_contests    529659\n",
      "taco              85774\n",
      "apps              81631\n",
      "-                 38191\n",
      "\n",
      "Value counts — license:\n",
      "license\n",
      "cc-by-4.0     529659\n",
      "apache-2.0     85774\n",
      "mit            81631\n",
      "-              38191\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if parquet_writer:\n",
    "    parquet_writer.close()\n",
    "\n",
    "print(f\"\\nStreaming complete!\")\n",
    "print(f\"Total rows : {total_rows:,}\")\n",
    "print(f\"Saved to   : {OUTPUT_PARQUET}\")\n",
    "\n",
    "# ## Cell 5: Reload from Parquet and preview\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "\n",
    "print(\"\\n--- Reloading from Parquet for inspection ---\")\n",
    "\n",
    "# Read with pyarrow directly, then convert — avoids the pandas/pyarrow compat bug\n",
    "table = pq.read_table(OUTPUT_PARQUET)\n",
    "df = table.to_pandas(safe=False)\n",
    "\n",
    "print(f\"Shape   : {df.shape}\")\n",
    "print(f\"Columns : {list(df.columns)}\")\n",
    "\n",
    "preview = df.head(5).copy()\n",
    "for col in [\"input\", \"solution\"]:\n",
    "    if col in preview.columns:\n",
    "        preview[col] = preview[col].str[:100] + \"...\"\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(preview.to_string(index=False))\n",
    "\n",
    "# ## Cell 6: Dataset statistics\n",
    "print(\"\\n--- Dataset Statistics ---\")\n",
    "for col in [\"source\", \"difficulty\", \"dataset\", \"license\"]:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\nValue counts — {col}:\")\n",
    "        print(df[col].value_counts().to_string())\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47aa503-bb26-4f66-8f7d-2a519929c15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4964d1d-6af7-4cbf-8621-c6a8427445b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parquet...\n",
      "Total rows : 735,255\n",
      "File size  : 772.6 MB\n",
      "Target     : 10 MB per part\n",
      "~Rows/chunk: 9,516\n",
      "  part_0000.json — 9,516 rows — 26.6 MB\n",
      "  part_0001.json — 9,516 rows — 26.6 MB\n",
      "  part_0002.json — 9,516 rows — 26.6 MB\n",
      "  part_0003.json — 9,516 rows — 26.6 MB\n",
      "  part_0004.json — 9,516 rows — 26.6 MB\n",
      "  part_0005.json — 9,516 rows — 26.5 MB\n",
      "  part_0006.json — 9,516 rows — 26.7 MB\n",
      "  part_0007.json — 9,516 rows — 26.6 MB\n",
      "  part_0008.json — 9,516 rows — 26.6 MB\n",
      "  part_0009.json — 9,516 rows — 26.5 MB\n",
      "  part_0010.json — 9,516 rows — 26.5 MB\n",
      "  part_0011.json — 9,516 rows — 26.6 MB\n",
      "  part_0012.json — 9,516 rows — 26.5 MB\n",
      "  part_0013.json — 9,516 rows — 26.5 MB\n",
      "  part_0014.json — 9,516 rows — 26.6 MB\n",
      "  part_0015.json — 9,516 rows — 26.7 MB\n",
      "  part_0016.json — 9,516 rows — 26.6 MB\n",
      "  part_0017.json — 9,516 rows — 26.6 MB\n",
      "  part_0018.json — 9,516 rows — 26.6 MB\n",
      "  part_0019.json — 9,516 rows — 26.5 MB\n",
      "  part_0020.json — 9,516 rows — 26.7 MB\n",
      "  part_0021.json — 9,516 rows — 26.5 MB\n",
      "  part_0022.json — 9,516 rows — 26.5 MB\n",
      "  part_0023.json — 9,516 rows — 26.7 MB\n",
      "  part_0024.json — 9,516 rows — 26.7 MB\n",
      "  part_0025.json — 9,516 rows — 26.7 MB\n",
      "  part_0026.json — 9,516 rows — 26.7 MB\n",
      "  part_0027.json — 9,516 rows — 26.5 MB\n",
      "  part_0028.json — 9,516 rows — 26.6 MB\n",
      "  part_0029.json — 9,516 rows — 26.7 MB\n",
      "  part_0030.json — 9,516 rows — 26.6 MB\n",
      "  part_0031.json — 9,516 rows — 26.7 MB\n",
      "  part_0032.json — 9,516 rows — 26.5 MB\n",
      "  part_0033.json — 9,516 rows — 26.7 MB\n",
      "  part_0034.json — 9,516 rows — 26.6 MB\n",
      "  part_0035.json — 9,516 rows — 26.7 MB\n",
      "  part_0036.json — 9,516 rows — 26.6 MB\n",
      "  part_0037.json — 9,516 rows — 26.6 MB\n",
      "  part_0038.json — 9,516 rows — 26.5 MB\n",
      "  part_0039.json — 9,516 rows — 26.7 MB\n",
      "  part_0040.json — 9,516 rows — 30.2 MB\n",
      "  part_0041.json — 9,516 rows — 36.7 MB\n",
      "  part_0042.json — 9,516 rows — 36.6 MB\n",
      "  part_0043.json — 9,516 rows — 36.7 MB\n",
      "  part_0044.json — 9,516 rows — 37.1 MB\n",
      "  part_0045.json — 9,516 rows — 36.7 MB\n",
      "  part_0046.json — 9,516 rows — 35.5 MB\n",
      "  part_0047.json — 9,516 rows — 27.0 MB\n",
      "  part_0048.json — 9,516 rows — 27.0 MB\n",
      "  part_0049.json — 9,516 rows — 26.9 MB\n",
      "  part_0050.json — 9,516 rows — 26.9 MB\n",
      "  part_0051.json — 9,516 rows — 31.7 MB\n",
      "  part_0052.json — 9,516 rows — 36.5 MB\n",
      "  part_0053.json — 9,516 rows — 36.3 MB\n",
      "  part_0054.json — 9,516 rows — 36.7 MB\n",
      "  part_0055.json — 9,516 rows — 36.4 MB\n",
      "  part_0056.json — 9,516 rows — 36.2 MB\n",
      "  part_0057.json — 9,516 rows — 36.4 MB\n",
      "  part_0058.json — 9,516 rows — 36.2 MB\n",
      "  part_0059.json — 9,516 rows — 26.6 MB\n",
      "  part_0060.json — 9,516 rows — 8.3 MB\n",
      "  part_0061.json — 9,516 rows — 7.9 MB\n",
      "  part_0062.json — 9,516 rows — 8.0 MB\n",
      "  part_0063.json — 9,516 rows — 7.9 MB\n",
      "  part_0064.json — 9,516 rows — 8.1 MB\n",
      "  part_0065.json — 9,516 rows — 7.7 MB\n",
      "  part_0066.json — 9,516 rows — 8.6 MB\n",
      "  part_0067.json — 9,516 rows — 7.7 MB\n",
      "  part_0068.json — 9,516 rows — 7.9 MB\n",
      "  part_0069.json — 9,516 rows — 8.1 MB\n",
      "  part_0070.json — 9,516 rows — 8.0 MB\n",
      "  part_0071.json — 9,516 rows — 8.0 MB\n",
      "  part_0072.json — 9,516 rows — 8.2 MB\n",
      "  part_0073.json — 9,516 rows — 7.8 MB\n",
      "  part_0074.json — 9,516 rows — 8.3 MB\n",
      "  part_0075.json — 9,516 rows — 7.9 MB\n",
      "  part_0076.json — 9,516 rows — 8.0 MB\n",
      "  part_0077.json — 2,523 rows — 2.2 MB\n",
      "\n",
      "Done! 78 files written to: opencodereasoning_parts/\n"
     ]
    }
   ],
   "source": [
    "# ## Split large Parquet into ~40MB chunks (JSON) for GitHub\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "INPUT_PARQUET = \"opencodereasoning_filtered.parquet\"\n",
    "OUTPUT_DIR    = \"opencodereasoning_parts\"\n",
    "TARGET_MB     = 10\n",
    "TARGET_BYTES  = TARGET_MB * 1024 * 1024\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load with pyarrow directly\n",
    "print(\"Reading parquet...\")\n",
    "table = pq.read_table(INPUT_PARQUET)\n",
    "total_rows = len(table)\n",
    "file_size  = os.path.getsize(INPUT_PARQUET)\n",
    "\n",
    "bytes_per_row  = file_size / total_rows\n",
    "rows_per_chunk = int(TARGET_BYTES / bytes_per_row)\n",
    "\n",
    "print(f\"Total rows : {total_rows:,}\")\n",
    "print(f\"File size  : {file_size / 1024**2:.1f} MB\")\n",
    "print(f\"Target     : {TARGET_MB} MB per part\")\n",
    "print(f\"~Rows/chunk: {rows_per_chunk:,}\")\n",
    "\n",
    "# Split and write as JSON\n",
    "part = 0\n",
    "for start in range(0, total_rows, rows_per_chunk):\n",
    "    chunk_df = table.slice(start, rows_per_chunk).to_pandas(safe=False)\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"part_{part:04d}.json\")\n",
    "    chunk_df.to_json(out_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "    size_mb = os.path.getsize(out_path) / 1024**2\n",
    "    print(f\"  part_{part:04d}.json — {len(chunk_df):,} rows — {size_mb:.1f} MB\")\n",
    "    part += 1\n",
    "\n",
    "print(f\"\\nDone! {part} files written to: {OUTPUT_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee3930-0c15-4617-8fc1-e342752c45b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641194b-cc43-4b92-9c0c-f76de7546b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb311382-c1f2-4cd7-83a5-79df6e6156ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading original parquet...\n",
      "Original : 735,255 rows, ['id', 'input', 'source', 'dataset', 'license', 'split', 'difficulty', 'solution']\n",
      "\n",
      "Found 78 JSON part files in 'opencodereasoning_parts/'\n",
      "Combined : 735,255 rows, ['id', 'input', 'source', 'dataset', 'license', 'split', 'difficulty', 'solution']\n",
      "\n",
      "--- Validation Checks ---\n",
      "  [PASS] Row count matches: 735,255\n",
      "  [PASS] Columns match: ['id', 'input', 'source', 'dataset', 'license', 'split', 'difficulty', 'solution']\n",
      "\n",
      "  Null counts (original vs combined):\n",
      "    [PASS] id: original=0  parts=0\n",
      "    [PASS] input: original=0  parts=0\n",
      "    [PASS] source: original=0  parts=0\n",
      "    [PASS] dataset: original=0  parts=0\n",
      "    [PASS] license: original=0  parts=0\n",
      "    [PASS] split: original=0  parts=0\n",
      "    [PASS] difficulty: original=0  parts=0\n",
      "    [PASS] solution: original=0  parts=0\n",
      "\n",
      "  [PASS] All IDs match (28,319 unique)\n",
      "\n",
      "--- Verdict ---\n",
      "  All checks passed! Safe to delete the original parquet.\n",
      "\n",
      "  To delete, run:  os.remove('opencodereasoning_filtered.parquet')\n"
     ]
    }
   ],
   "source": [
    "# ## Validate Parquet vs JSON parts before deleting the original\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "INPUT_PARQUET = \"opencodereasoning_filtered.parquet\"\n",
    "OUTPUT_DIR    = \"opencodereasoning_parts\"\n",
    "\n",
    "# --- Step 1: Load original parquet ---\n",
    "print(\"Reading original parquet...\")\n",
    "table      = pq.read_table(INPUT_PARQUET)\n",
    "df_orig    = table.to_pandas(safe=False)\n",
    "total_rows = len(df_orig)\n",
    "print(f\"Original : {total_rows:,} rows, {list(df_orig.columns)}\")\n",
    "\n",
    "# --- Step 2: Load all JSON parts ---\n",
    "json_files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.endswith(\".json\")])\n",
    "print(f\"\\nFound {len(json_files)} JSON part files in '{OUTPUT_DIR}/'\")\n",
    "\n",
    "df_parts = pd.concat(\n",
    "    [pd.read_json(os.path.join(OUTPUT_DIR, f), lines=True) for f in json_files],\n",
    "    ignore_index=True\n",
    ")\n",
    "print(f\"Combined : {len(df_parts):,} rows, {list(df_parts.columns)}\")\n",
    "\n",
    "# --- Step 3: Checks ---\n",
    "print(\"\\n--- Validation Checks ---\")\n",
    "checks_passed = True\n",
    "\n",
    "# Row count\n",
    "if len(df_parts) == total_rows:\n",
    "    print(f\"  [PASS] Row count matches: {total_rows:,}\")\n",
    "else:\n",
    "    print(f\"  [FAIL] Row count mismatch — original: {total_rows:,}, parts: {len(df_parts):,}\")\n",
    "    checks_passed = False\n",
    "\n",
    "# Column match\n",
    "if set(df_parts.columns) == set(df_orig.columns):\n",
    "    print(f\"  [PASS] Columns match: {list(df_orig.columns)}\")\n",
    "else:\n",
    "    print(f\"  [FAIL] Column mismatch — original: {set(df_orig.columns)}, parts: {set(df_parts.columns)}\")\n",
    "    checks_passed = False\n",
    "\n",
    "# Null counts per column\n",
    "print(\"\\n  Null counts (original vs combined):\")\n",
    "null_match = True\n",
    "for col in df_orig.columns:\n",
    "    n_orig  = df_orig[col].isna().sum()\n",
    "    n_parts = df_parts[col].isna().sum()\n",
    "    status  = \"PASS\" if n_orig == n_parts else \"FAIL\"\n",
    "    if status == \"FAIL\":\n",
    "        null_match = False\n",
    "        checks_passed = False\n",
    "    print(f\"    [{status}] {col}: original={n_orig:,}  parts={n_parts:,}\")\n",
    "\n",
    "# Sample content check — compare first and last 5 rows by 'id'\n",
    "if \"id\" in df_orig.columns:\n",
    "    orig_ids  = set(df_orig[\"id\"].astype(str))\n",
    "    parts_ids = set(df_parts[\"id\"].astype(str))\n",
    "    if orig_ids == parts_ids:\n",
    "        print(f\"\\n  [PASS] All IDs match ({len(orig_ids):,} unique)\")\n",
    "    else:\n",
    "        missing  = orig_ids - parts_ids\n",
    "        extra    = parts_ids - orig_ids\n",
    "        print(f\"\\n  [FAIL] ID mismatch — missing: {len(missing):,}, extra: {len(extra):,}\")\n",
    "        checks_passed = False\n",
    "\n",
    "# --- Step 4: Verdict ---\n",
    "print(\"\\n--- Verdict ---\")\n",
    "if checks_passed:\n",
    "    print(\"  All checks passed! Safe to delete the original parquet.\")\n",
    "    print(f\"\\n  To delete, run:  os.remove('{INPUT_PARQUET}')\")\n",
    "    # Uncomment the line below to auto-delete:\n",
    "    # os.remove(INPUT_PARQUET)\n",
    "else:\n",
    "    print(\"  One or more checks FAILED. Do NOT delete the original parquet yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ddf2ccc-f004-43fd-abd1-1c5c8daaedfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted opencodereasoning_filtered.parquet\n"
     ]
    }
   ],
   "source": [
    "os.remove(\"opencodereasoning_filtered.parquet\")\n",
    "print(\"Deleted opencodereasoning_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90690fe-a8ec-4540-9e58-086e48be29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values check...\n",
      "  input    : 28,319 unique out of 735,255 rows\n",
      "  solution : 626,241 unique out of 735,255 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values check...\")\n",
    "print(f\"  input    : {df_orig['id'].nunique():,} unique out of {len(df_orig):,} rows\")\n",
    "print(f\"  solution : {df_orig['solution'].nunique():,} unique out of {len(df_orig):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69feba38-678e-44f7-95ae-c06a01b5ec7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(735255, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7266a-ab3f-45ae-b6d8-c67224f15ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "building",
   "language": "python",
   "name": "building"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
